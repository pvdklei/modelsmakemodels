{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_sebas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvdklei/modelsmakemodels/blob/master/sebas/Autoencoder%2Bclassifier_sebas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgoqvG3DmJPs"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import train\n",
        "import utils"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLhIT2X3mWxn",
        "outputId": "af5092cc-2a2d-47d5-af68-972894b4e364"
      },
      "source": [
        "traintransform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "testtransform = traintransform\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "trainset = datasets.CIFAR10(\"/data/cifar10/train\", train=True, transform=traintransform, download=True)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "testset = datasets.CIFAR10(\"/data/cifar10/test\", train=False, transform=testtransform, download=True)\n",
        "testloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEK10STTnWDk",
        "outputId": "8fb03912-6102-4951-b3ff-43a78bbb711f"
      },
      "source": [
        "imiter = iter(trainloader)\n",
        "images, _ = next(imiter)\n",
        "image = images[0]\n",
        "images.shape\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "bcIX4poac29Y",
        "outputId": "a6440b18-6ed2-4501-d1b1-9baa1ce15599"
      },
      "source": [
        "def showimage(image):\r\n",
        "    image = image.detach().numpy()\r\n",
        "    image = image.transpose((1, 2, 0))\r\n",
        "    plt.imshow(image)\r\n",
        "showimage(torchvision.utils.make_grid(images[0]))\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0l4S7yimc-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48562c1-8b30-4899-eb2e-aa5bcbc862f6"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 12, kernel_size=4, stride=2, padding = 1)\n",
        "        self.conv2 = nn.Conv2d(12, 24, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(24, 48, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        return x\n",
        "        \n",
        "enc = Encoder()\n",
        "enc(images).shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 48, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH4kR8w51b4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3cbebd-6709-4e59-ddc9-989bf0e52040"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(48, 24, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(24, 12, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.ConvTranspose2d(12, 3, kernel_size=4, stride=2, padding=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.sigmoid(self.conv3(x))\n",
        "        return x\n",
        "\n",
        "decoder = Decoder()\n",
        "decoder(enc(images)).shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olAnjSdn1hbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3b31b6-3b6e-4a0e-f499-26e72400b494"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "autoenc = AutoEncoder()\n",
        "autoenc(images).shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5gmDOQPlgT2",
        "outputId": "195ff982-e234-47e3-a749-cbb2e91f280e"
      },
      "source": [
        "model = AutoEncoder()\n",
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder): Encoder(\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (conv1): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (conv1): ConvTranspose2d(48, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv2): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): ConvTranspose2d(12, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9sRBJ2GM_XR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d754598b-8b66-4e5b-dfac-528bba712083"
      },
      "source": [
        "state_dict = torch.load(\"checkpoint.model\")\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN01gXhY8-C2",
        "outputId": "0d99e026-a308-49d5-970e-25b290ba2109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class Classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.fc1 = nn.Linear(48*4*4, 10) \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "        x = self.fc1(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "mod = Classifier()\r\n",
        "mod(enc(images)).shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNWPYZcy9bK1"
      },
      "source": [
        "class AutoEncoder_classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.encoder = Encoder()\r\n",
        "        self.classifier = Classifier()\r\n",
        "    def forward(self, x):\r\n",
        "        with torch.no_grad():\r\n",
        "            x = self.encoder(x)\r\n",
        "        x = self.classifier(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "class_enc = AutoEncoder_classifier()\r\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T5FcNweBsMT",
        "outputId": "6e85c7c2-8f9c-4125-e514-499ba69e8425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = AutoEncoder_classifier()\r\n",
        "model"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder_classifier(\n",
              "  (encoder): Encoder(\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (conv1): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (fc1): Linear(in_features=768, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtSKtPhvDrEk"
      },
      "source": [
        "state_dict = torch.load(\"checkpoint.model\")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq85JNaNDaNh"
      },
      "source": [
        "from torch.nn.parameter import Parameter\r\n",
        "def load_my_state_dict(self, state_dict):\r\n",
        "\r\n",
        "    own_state = self.state_dict()\r\n",
        "    for name, param in state_dict.items():\r\n",
        "        if name not in own_state:\r\n",
        "              continue\r\n",
        "        if isinstance(param, Parameter):\r\n",
        "            # backwards compatibility for serialized parameters\r\n",
        "            param = param.data\r\n",
        "        own_state[name].copy_(param)\r\n",
        "load_my_state_dict(model, state_dict)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5LRPNH5C3sH"
      },
      "source": [
        "def train(model, optimizer, trainloader, testloader, criterion=nn.CrossEntropyLoss(), epochs=5):\r\n",
        "\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    print(device)\r\n",
        "\r\n",
        "    lowest_testloss = np.Inf\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "\r\n",
        "        # training\r\n",
        "        trainloss = 0\r\n",
        "        for images, labels in trainloader:\r\n",
        "            images, labels = images.to(device), labels.to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            out = model(images)\r\n",
        "            loss = criterion(out, labels)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            trainloss += loss.item()\r\n",
        "        print(f\"Epoch {epoch}, Training loss: {trainloss}\")\r\n",
        "\r\n",
        "        # validation\r\n",
        "        testloss = 0\r\n",
        "        for images, labels in testloader:\r\n",
        "            images, labels = images.to(device), labels.to(device)\r\n",
        "            with torch.no_grad():\r\n",
        "                out = model(images)\r\n",
        "                loss = criterion(out, labels)\r\n",
        "                testloss += loss\r\n",
        "        print(f\"Epoch {epoch}, Validation loss: {testloss}\")\r\n",
        "\r\n",
        "\r\n",
        "        lowest_testloss = testloss\r\n",
        "    \r\n",
        "    # final accuracy\r\n",
        "    totacc = 0\r\n",
        "    n = 0\r\n",
        "    for images, labels in testloader:\r\n",
        "        images, labels = images.to(device), labels.to(device)\r\n",
        "        with torch.no_grad():\r\n",
        "            out = model(images)\r\n",
        "            topv, topi = torch.topk(out, 1, dim=1)\r\n",
        "            labels.resize_(*topi.shape)\r\n",
        "            eq = topi == labels\r\n",
        "            acc = torch.mean(eq.type(torch.FloatTensor))\r\n",
        "            totacc += acc.item()\r\n",
        "            n += 1\r\n",
        "    totacc /= n\r\n",
        "    print(f\"The final total accuracy is: {totacc * 100}\")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfi5dL52FxrI",
        "outputId": "84017448-d67c-4665-b00e-25bbf047d980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train(model, \r\n",
        "      optim.SGD(params=model.parameters(), lr=0.005),\r\n",
        "      trainloader, \r\n",
        "      testloader, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch 0, Training loss: 16974.97495317459\n",
            "Epoch 0, Validation loss: 18635.681640625\n",
            "Epoch 1, Training loss: 13856.654603481293\n",
            "Epoch 1, Validation loss: 15937.3876953125\n",
            "Epoch 2, Training loss: 13090.264585494995\n",
            "Epoch 2, Validation loss: 16929.119140625\n",
            "Epoch 3, Training loss: 13096.683172225952\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}