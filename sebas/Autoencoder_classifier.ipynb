{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_sebas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvdklei/modelsmakemodels/blob/master/sebas/Autoencoder_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgoqvG3DmJPs"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import train\n",
        "import utils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLhIT2X3mWxn",
        "outputId": "6ec8e435-379d-4bbb-f0ae-d116ce5032f5"
      },
      "source": [
        "traintransform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "testtransform = traintransform\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "trainset = datasets.CIFAR10(\"/data/cifar10/train\", train=True, transform=traintransform, download=True)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "testset = datasets.CIFAR10(\"/data/cifar10/test\", train=False, transform=testtransform, download=True)\n",
        "testloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEK10STTnWDk",
        "outputId": "b693c706-4261-46ca-a55c-fbe12feb04d7"
      },
      "source": [
        "imiter = iter(trainloader)\n",
        "images, _ = next(imiter)\n",
        "image = images[0]\n",
        "images.shape\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "bcIX4poac29Y",
        "outputId": "075ebfc7-670c-43bb-ec3e-7981967573ba"
      },
      "source": [
        "def showimage(image):\r\n",
        "    image = image.detach().numpy()\r\n",
        "    image = image.transpose((1, 2, 0))\r\n",
        "    plt.imshow(image)\r\n",
        "showimage(torchvision.utils.make_grid(images[0]))\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0l4S7yimc-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59e175a-9041-42fd-9a4b-fa864746f726"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 12, kernel_size=4, stride=2, padding = 1)\n",
        "        self.conv2 = nn.Conv2d(12, 24, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(24, 48, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        return x\n",
        "        \n",
        "enc = Encoder()\n",
        "enc(images).shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 48, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH4kR8w51b4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad39ce4e-cc91-4f0b-e165-215cdeca3010"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(48, 24, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(24, 12, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.ConvTranspose2d(12, 3, kernel_size=4, stride=2, padding=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.sigmoid(self.conv3(x))\n",
        "        return x\n",
        "\n",
        "decoder = Decoder()\n",
        "decoder(enc(images)).shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olAnjSdn1hbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "820bb109-977c-4e92-8e03-596c6ca82cfb"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "autoenc = AutoEncoder()\n",
        "autoenc(images).shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5gmDOQPlgT2",
        "outputId": "59ab8a8f-3f0d-4d16-f072-2d953a064eef"
      },
      "source": [
        "model = AutoEncoder()\n",
        "model"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder): Encoder(\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (conv1): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (conv1): ConvTranspose2d(48, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv2): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): ConvTranspose2d(12, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9sRBJ2GM_XR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34477f01-6861-4e97-9f1f-a619abec708f"
      },
      "source": [
        "state_dict = torch.load(\"checkpoint.model\")\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN01gXhY8-C2",
        "outputId": "70d90fb8-f6d1-4652-950a-2265c6763769"
      },
      "source": [
        "class Classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.fc1 = nn.Linear(48*4*4, 10) \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "        x = self.fc1(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "mod = Classifier()\r\n",
        "mod(enc(images)).shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNWPYZcy9bK1"
      },
      "source": [
        "class AutoEncoder_classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.encoder = Encoder()\r\n",
        "        self.classifier = Classifier()\r\n",
        "    def forward(self, x):\r\n",
        "        with torch.no_grad():\r\n",
        "            x = self.encoder(x)\r\n",
        "        x = self.classifier(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "class_enc = AutoEncoder_classifier()\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T5FcNweBsMT",
        "outputId": "042f4d2c-0ee1-4ce4-b3d6-71972fc6b2ed"
      },
      "source": [
        "model = AutoEncoder_classifier()\r\n",
        "model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder_classifier(\n",
              "  (encoder): Encoder(\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (conv1): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (fc1): Linear(in_features=768, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtSKtPhvDrEk"
      },
      "source": [
        "state_dict = torch.load(\"checkpoint.model\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq85JNaNDaNh"
      },
      "source": [
        "from torch.nn.parameter import Parameter\r\n",
        "def load_my_state_dict(self, state_dict):\r\n",
        "\r\n",
        "    own_state = self.state_dict()\r\n",
        "    for name, param in state_dict.items():\r\n",
        "        if name not in own_state:\r\n",
        "              continue\r\n",
        "        if isinstance(param, Parameter):\r\n",
        "            # backwards compatibility for serialized parameters\r\n",
        "            param = param.data\r\n",
        "        own_state[name].copy_(param)\r\n",
        "load_my_state_dict(model, state_dict)\r\n",
        "\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5LRPNH5C3sH"
      },
      "source": [
        "def train(model, optimizer, trainloader, testloader, criterion=nn.CrossEntropyLoss(), epochs=5):\r\n",
        "\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    print(device)\r\n",
        "\r\n",
        "    lowest_testloss = np.Inf\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "\r\n",
        "        # training\r\n",
        "        trainloss = 0\r\n",
        "        for images, labels in trainloader:\r\n",
        "            images, labels = images.to(device), labels.to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            out = model(images)\r\n",
        "            loss = criterion(out, labels)\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            trainloss += loss.item()\r\n",
        "        print(f\"Epoch {epoch}, Training loss: {trainloss/len(trainloader)}\")\r\n",
        "\r\n",
        "        # validation\r\n",
        "        testloss = 0\r\n",
        "        for images, labels in testloader:\r\n",
        "            images, labels = images.to(device), labels.to(device)\r\n",
        "            with torch.no_grad():\r\n",
        "                out = model(images)\r\n",
        "                loss = criterion(out, labels)\r\n",
        "                testloss += loss\r\n",
        "        print(f\"Epoch {epoch}, Validation loss: {testloss/len(testloader)}\")\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "        lowest_testloss = testloss\r\n",
        "    \r\n",
        "    # final accuracy\r\n",
        "    totacc = 0\r\n",
        "    n = 0\r\n",
        "    for images, labels in testloader:\r\n",
        "        images, labels = images.to(device), labels.to(device)\r\n",
        "        with torch.no_grad():\r\n",
        "            out = model(images)\r\n",
        "            topv, topi = torch.topk(out, 1, dim=1)\r\n",
        "            labels.resize_(*topi.shape)\r\n",
        "            eq = topi == labels\r\n",
        "            acc = torch.mean(eq.type(torch.FloatTensor))\r\n",
        "            totacc += acc.item()\r\n",
        "            n += 1\r\n",
        "    totacc = n\r\n",
        "    print(f\"The final total accuracy is: {totacc * 100}\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfi5dL52FxrI",
        "outputId": "62f2e9c8-6c56-42c8-f40e-05d9d47ca067"
      },
      "source": [
        "train(model, \r\n",
        "      optim.Adam(params=model.classifier.parameters(), lr=0.001),\r\n",
        "      trainloader, \r\n",
        "      testloader, epochs=40)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch 0, Training loss: 1.9344729619078047\n",
            "Epoch 0, Validation loss: 1.871349573135376\n",
            "Epoch 1, Training loss: 1.9415711998863245\n",
            "Epoch 1, Validation loss: 1.8313673734664917\n",
            "Epoch 2, Training loss: 1.9051649142592975\n",
            "Epoch 2, Validation loss: 1.8893557786941528\n",
            "Epoch 3, Training loss: 1.9309906576660605\n",
            "Epoch 3, Validation loss: 1.8504259586334229\n",
            "Epoch 4, Training loss: 1.9046103502799514\n",
            "Epoch 4, Validation loss: 1.7836047410964966\n",
            "Epoch 5, Training loss: 1.8981830642838091\n",
            "Epoch 5, Validation loss: 1.9689297676086426\n",
            "Epoch 6, Training loss: 1.8989581964142568\n",
            "Epoch 6, Validation loss: 1.9044287204742432\n",
            "Epoch 7, Training loss: 1.9197154187729017\n",
            "Epoch 7, Validation loss: 1.9624512195587158\n",
            "Epoch 8, Training loss: 1.8874044674600612\n",
            "Epoch 8, Validation loss: 1.8700913190841675\n",
            "Epoch 9, Training loss: 1.8951141053258953\n",
            "Epoch 9, Validation loss: 1.8448433876037598\n",
            "Epoch 10, Training loss: 1.8925918194245468\n",
            "Epoch 10, Validation loss: 2.096052646636963\n",
            "Epoch 11, Training loss: 1.882527650966144\n",
            "Epoch 11, Validation loss: 1.8286893367767334\n",
            "Epoch 12, Training loss: 1.8822205377860628\n",
            "Epoch 12, Validation loss: 1.7779191732406616\n",
            "Epoch 13, Training loss: 1.8691800294285468\n",
            "Epoch 13, Validation loss: 1.935776948928833\n",
            "Epoch 14, Training loss: 1.8909490755255682\n",
            "Epoch 14, Validation loss: 1.7094272375106812\n",
            "Epoch 15, Training loss: 1.8913545874701199\n",
            "Epoch 15, Validation loss: 1.946221113204956\n",
            "Epoch 16, Training loss: 1.881440046576453\n",
            "Epoch 16, Validation loss: 1.9350974559783936\n",
            "Epoch 17, Training loss: 1.885937841367203\n",
            "Epoch 17, Validation loss: 1.8903642892837524\n",
            "Epoch 18, Training loss: 1.8808283928641125\n",
            "Epoch 18, Validation loss: 1.919796347618103\n",
            "Epoch 19, Training loss: 1.864526803495025\n",
            "Epoch 19, Validation loss: 1.7482388019561768\n",
            "Epoch 20, Training loss: 1.8712310461912565\n",
            "Epoch 20, Validation loss: 1.945176124572754\n",
            "Epoch 21, Training loss: 1.8728043100274074\n",
            "Epoch 21, Validation loss: 1.8213932514190674\n",
            "Epoch 22, Training loss: 1.871108719308027\n",
            "Epoch 22, Validation loss: 1.7932766675949097\n",
            "Epoch 23, Training loss: 1.863004778107236\n",
            "Epoch 23, Validation loss: 1.8184142112731934\n",
            "Epoch 24, Training loss: 1.8827848075981408\n",
            "Epoch 24, Validation loss: 1.6759752035140991\n",
            "Epoch 25, Training loss: 1.8714604211478987\n",
            "Epoch 25, Validation loss: 1.805943250656128\n",
            "Epoch 26, Training loss: 1.863982685849366\n",
            "Epoch 26, Validation loss: 1.8125895261764526\n",
            "Epoch 27, Training loss: 1.8786760371843363\n",
            "Epoch 27, Validation loss: 1.9489796161651611\n",
            "Epoch 28, Training loss: 1.8633293127754293\n",
            "Epoch 28, Validation loss: 1.9640369415283203\n",
            "Epoch 29, Training loss: 1.882475322054047\n",
            "Epoch 29, Validation loss: 1.9050307273864746\n",
            "Epoch 30, Training loss: 1.8754860139854124\n",
            "Epoch 30, Validation loss: 1.844563364982605\n",
            "Epoch 31, Training loss: 1.867239387769083\n",
            "Epoch 31, Validation loss: 1.738034725189209\n",
            "Epoch 32, Training loss: 1.8515027925057512\n",
            "Epoch 32, Validation loss: 1.7866501808166504\n",
            "Epoch 33, Training loss: 1.8499542560931284\n",
            "Epoch 33, Validation loss: 1.801154613494873\n",
            "Epoch 34, Training loss: 1.8686126311162459\n",
            "Epoch 34, Validation loss: 1.860682725906372\n",
            "Epoch 35, Training loss: 1.8566955806960377\n",
            "Epoch 35, Validation loss: 1.8739078044891357\n",
            "Epoch 36, Training loss: 1.860625391195618\n",
            "Epoch 36, Validation loss: 1.7785463333129883\n",
            "Epoch 37, Training loss: 1.8527845463078523\n",
            "Epoch 37, Validation loss: 1.8007380962371826\n",
            "Epoch 38, Training loss: 1.8768292798190565\n",
            "Epoch 38, Validation loss: 1.9907578229904175\n",
            "Epoch 39, Training loss: 1.8455413232151698\n",
            "Epoch 39, Validation loss: 1.737767219543457\n",
            "The final total accuracy is: 156300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RybTuNuJ11D4"
      },
      "source": [
        "torch.save(model.state_dict(), 'checkpoint_auto_classifier.model')"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}